# -*- coding: utf-8 -*-
"""HR assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jjh0k8TdRGmOKjvx4UJAyJJZt_2qb5rm

#Introduction

A data consulting/ analytics company has asked us to develop a model to predict why high performing employees are leaving at a higher rate. This notebook will process the data given and the markdowns will explain what is happening and what insights can be drawn here.

#Data

##Imports

This section is simply used to import the libraries necessary to determine the model of the problem. As well as importing the dataset that Bob has provided us with.
"""

import pandas as pd
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

drive.mount('/content/Drive')

hr_data=pd.read_csv('/content/Drive/My Drive/hr-data.csv',low_memory=False)

"""##Exploratory Data Analysis

Now that the data has been imported into the notebook, we will now be performing exploratory data analysis on it.
"""

hr_data.info()

hr_data.shape

"""The dataset contains 11 columns of variables listed above and has 14999 entries. The satisfaction_level and last_evaluation is a float variable, whereas sales and salary are object types. Everything else is integer type."""

#sample of what the data looks like
hr_data

hr_data.isnull().sum()

hr_data.nunique()

#types of variables
categorical_variables = [
    'number_project',
    'time_spend_company',
    'Work_accident',
    'promotion_last_5years'
    'sales'
    'salary'
]

numerical_variables = [
    'satisfaction_level',
    'last_evaluation',
    'average_montly_hours'
]

target = ['churn']

"""There are no null values in the dataset. The columns has been sorted according to their variable types. Categorical and numerical data. The target data is the churn column for employees if the employee has left."""

hr_data_dm=pd.get_dummies(hr_data, columns=['sales','salary'])
hr_data_dm.info()

"""Create a new dataframe for dummy variables to be applied for sales and salary column."""

#descriptive statistics of following columns
hr_data[['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company']].describe()

hr_data['churn'].value_counts()

hr_data['churn'].value_counts(normalize=True)

"""The percentage in people churning from the Company is 23.8% with 3571. Whereas the ones who stayed are 76.2% with 11428."""

hr_data.groupby('churn').mean()

"""We grouped the data by churn and averaged the data and here are my observations:

- The average satisfaction level of the employees of the ones who did not churn are higher than the ones who did. 
- The number of times employees have been promoted in the last 5 years are higher than the ones who did. 
- The mean of average monthly hours are shorter for the ones who stayed compared to the ones who left but not by a lot.
- There are more work accidents for the ones who stayed.
- The average of last evaluation, number project and time spend company are very close to each other.
"""

hr_data.groupby('sales').mean()

hr_data.groupby('salary').mean()

"""We also calulated categroical means for other sales and salary to get ma more detailed sense of our data.
- satisfaction_levels, last_evaluation, average_monthly_hours are very close to each other.
- promotion_last_5years data varies in different groupings.

###Data visualization

Here is a data visualization of the data. Every column that involves a 0 and 1 as its values means no and yes respectively.
"""

#bar plot of the churn count.
sb.countplot(x='churn',data=hr_data)
plt.show()

#function for bar plots of churn frequency for the categorical data 
def bar_graph(column):
  pd.crosstab(hr_data[column], hr_data['churn']).plot(kind='bar')
  plt.title('Churn type frequency for '+column)
  plt.xlabel(column)
  plt.ylabel('Fequency of Churns')
  return

bar_graph('sales')

bar_graph('salary')

bar_graph('number_project')

#time spent at the company by years
bar_graph('time_spend_company')

bar_graph('Work_accident')

bar_graph('promotion_last_5years')

"""In all the bar charts above, the only time the yes churn exceeds the no is number_project 2. Raises question as to what's happening there."""

#function for distribution plots for the numerical data and 3 box and whiskers plot of the column.
# The first box and whiskers plot represents all of the satisfaction_level
# The other 2 boxes are filtered by the churn type
def continous_graph(column):
  fig, axs = plt.subplots(2,2,figsize=(10,10))
  sb.distplot(hr_data.loc[hr_data['churn']==0,column],bins=10, label='0', ax=axs[0][0])
  sb.distplot(hr_data.loc[hr_data['churn']==1,column],bins=10, label='1',ax=axs[0][0])
  sb.boxplot(x=column, data=hr_data,ax=axs[0][1])
  sb.boxplot(x=column, data=hr_data.loc[hr_data['churn']==1, :],ax=axs[1][0])
  sb.boxplot(x=column, data=hr_data.loc[hr_data['churn']==0, :],ax=axs[1][1])
  return

continous_graph('satisfaction_level')

continous_graph('last_evaluation')

continous_graph('average_montly_hours')

"""In the multiple graphs of the continous data above, we can see that the ones who churns are more in the lower end of the graph where as the ones who stays are in the higher end of the respective spectrum. The interesting thing about these representation is that even though the some of the means calculated earlier grouped by churn, the ones who ones who left are split on both ends where as the ones who stayed are moreso groupbed together.

##Modeling

Here we start modeling by taking the dataset with the created dummy variables and we perform features selection by splitting data into train and test size, 80% and 20%, scale the data, fit it into the Logistic Regression and look at the coefficient.
"""

hr_data_dm.columns

x_variables=['satisfaction_level', 'last_evaluation', 'number_project',
       'average_montly_hours', 'time_spend_company', 'Work_accident',
       'promotion_last_5years', 'sales_IT', 'sales_RandD', 'sales_accounting',
       'sales_hr', 'sales_management', 'sales_marketing', 'sales_product_mng',
       'sales_sales', 'sales_support', 'sales_technical', 'salary_high',
       'salary_low', 'salary_medium']

x = hr_data_dm[x_variables]
y = hr_data_dm['churn']

#feature selection
logreg=LogisticRegression(penalty='none',solver='lbfgs',verbose=1)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

x_train.shape

logreg.fit(x_train,y_train)

x_train_scaler=StandardScaler()

#fit scaler
x_train_scaler.fit(x_train)

#transform the data
x_train_scaled = x_train_scaler.transform(x_train)

logreg.fit(x_train_scaled,y_train)

coef=logreg.coef_
coef

x_train.columns

"""We'll use any coefficients greater than 0.10 on both negative and positive. So that leaves us with the following variables
- satisfaction_level
- last_evaluation
- number_project
- average_monthly_hours
- time_spend_company
- work_accident
- promotion_last_5years
- sales_RandD
- salary_high
- salary_low.

We go back plugging the data only this time with the ones the listed variables above and the different type of Logistic Regression.
"""

x_variables_feature=['satisfaction_level', 'last_evaluation', 'number_project',
       'average_montly_hours', 'time_spend_company', 'Work_accident',
       'promotion_last_5years', 'sales_RandD', 'salary_high',
       'salary_low']

x2=hr_data_dm[x_variables_feature]
y2=hr_data_dm['churn']

x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.2, random_state=42)

x2_train_scaler=StandardScaler()
cript of the notebook hr_assignment.py
#fit scaler
x2_train_scaler.fit(x2_train)

#transform the data
x2_train_scaled = x2_train_scaler.transform(x2_train)

# No Penalty
logreg2 = LogisticRegression(penalty='none',solver='lbfgs',verbose=1)

logreg2.fit(x2_train, y2_train)

y2_pred = logreg2.predict(x2_test)
logreg2.score(x2_test, y2_test)

logreg2.classes_

logreg2.predict_proba(x2_train)

print(classification_report(y_test, y2_pred))

confusion_matrix(y2_test,y2_pred)

logreg3 = LogisticRegression(penalty='l1',solver='saga',verbose=1)
logreg4 = LogisticRegression(penalty='l2',solver='lbfgs',verbose=1)
logreg5 = LogisticRegression(penalty='elasticnet',class_weight='balanced',solver='saga',verbose=1)

#Logistic Regression with L1 Penalty
logreg3.fit(x2_train, y2_train)

logreg3.score(x2_test, y2_test)

logreg3.predict_proba(x2_train)

y3_pred = logreg3.predict(x2_test)

print(classification_report(y2_test, y3_pred))

#Logistic Regression with L2 Penalty
logreg4.fit(x2_train, y2_train)

logreg4.score(x2_test, y2_test)

logreg4.predict_proba(x2_train)

y4_pred = logreg4.predict(x2_test)

print(classification_report(y2_test, y4_pred))

"""There was an error trying to create a logistic regression with elasticnet as penalty.

#Conclusions

After running the different types of regression models, with different types of penalty and solver, we can say that we can predict accurately around 78% percent accurately whether the employee will churn or not. The Variables that influence it are the following 
- satisfaction_level
- last_evaluation
- number_project
- average_monthly_hours
- time_spend_company
- work_accident
- promotion_last_5years
- sales_RandD
- salary_high
- salary_low.

The thing missing about this is the external factors that affect the employees decision. In the graph with the satisfactory, there are some who are pretty satisfied but left the company.
"""

